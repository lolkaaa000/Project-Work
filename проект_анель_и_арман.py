# -*- coding: utf-8 -*-
"""проект Анель и Арман

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MRpgGs32Y1D3UKK5taGiGAjz1P5RUgqc

# Project work on the subject "Fundamentals of Data Mining and Machine Learning"

**By:** Yerniyaz Arman and Amandos Anel

**Chapter 1.** Data loading and description

1.1 Loading data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv("framingham_heart_disease.csv")
df

"""1.2 Basic information about the data"""

df.shape

df.info()

df.describe()

"""1.3. Checking gaps"""

df.isnull().sum()

"""1.4 Checking for missing values"""

df["TenYearCHD"].value_counts()

"""**Chapter 2.** Data preprocessing"""

from sklearn.model_selection import train_test_split  # only for separation
from sklearn.preprocessing import StandardScaler      # for normalization

"""2.2 Handling missing values"""

# list of numeric features
num_cols = ["age","cigsPerDay","totChol","sysBP","diaBP","BMI","heartRate","glucose"]

# list of categorical features
cat_cols = ["education","BPMeds"]

# fill in the numbers with the average
for col in num_cols:
    df[col].fillna(df[col].mean(), inplace=True)

# fill in the categorical fashion
for col in cat_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

df.isnull().sum()

"""2.3 Selection and normalization of features and target variable"""

X = df.drop("TenYearCHD", axis=1).values  # all the signs
y = df["TenYearCHD"].values               # target variable

scaler = StandardScaler()
X = scaler.fit_transform(X)

"""2.4 Splitting into training and test samples"""

# 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""**Chapter 3.** Linear Regression

3.1 Defining functions

Loss function (MSE) and gradients:
"""

def compute_loss(y, y_pred):
    return np.mean((y - y_pred)**2)

def compute_gradient(X, y, y_pred):
    m = len(y)
    # gradients for weights and bias
    dw = (-2/m) * X.T.dot(y - y_pred)
    db = (-2/m) * np.sum(y - y_pred)/m
    return dw, db

"""3.2 Gradient descent"""

def linear_regression_gd(X, y, lr=0.01, epochs=1000):
    n_features = X.shape[1]
    w = np.zeros(n_features)
    b = 0
    losses = []

    for i in range(epochs):
        y_pred = X.dot(w) + b
        loss = compute_loss(y, y_pred)
        losses.append(loss)
        dw, db = compute_gradient(X, y, y_pred)
        w -= lr * dw
        b -= lr * db

    return w, b, losses

"""3.3 Model training"""

w, b, losses = linear_regression_gd(X_train, y_train, lr=0.01, epochs=1000)

"""3.4 Loss function graph"""

plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("MSE")
plt.title("Convergence of gradient descent")
plt.show()

"""3.5 Testing on a test sample"""

y_test_pred = X_test.dot(w) + b
test_loss = compute_loss(y_test, y_test_pred)
print("MSE on the test sample:", test_loss)

"""3.6 Coefficients and bias"""

print("Coefficients w:", w)
print("Bias b:", b)

"""**Chapter 4.** Visualization of linear regression

4.1. Scatter plot with line regression
"""

# We take the age sign
X_plot = X_test[:, 1]  # column age (index 1, since the order is the same as in X_train)
y_true = y_test

# Predictions by age only (for a visual 2D chart)
# To do this, we fix the remaining features to 0 (since they are normalized)
X_age_only = np.zeros((X_test.shape[0], X_test.shape[1]))
X_age_only[:, 1] = X_plot
y_pred_plot = X_age_only.dot(w) + b

# Graph
plt.figure(figsize=(8,5))
plt.scatter(X_plot, y_true, color='blue', alpha=0.5, label='Real values')
plt.plot(X_plot, y_pred_plot, color='red', label='Line regression')
plt.xlabel("Age (normalized)")
plt.ylabel("TenYearCHD")
plt.title("Linear Regression: Age vs TenYearCHD")
plt.legend()
plt.show()

"""4.2 Confidence interval (example ±1 std)"""

# Standard deviation of errors
residuals = y_true - y_pred_plot
std = np.std(residuals)

# Confidence interval ±1 std
upper = y_pred_plot + std
lower = y_pred_plot - std

plt.figure(figsize=(8,5))
plt.scatter(X_plot, y_true, color='blue', alpha=0.5, label='Real values')
plt.plot(X_plot, y_pred_plot, color='red', label='Regression line')
plt.fill_between(X_plot, lower, upper, color='orange', alpha=0.3, label='Confidence interval ±1σ')
plt.xlabel("Age (normalized)")
plt.ylabel("TenYearCHD")
plt.title("Linear regression with confidence interval")
plt.legend()
plt.show()

"""**Chapter 5.** Logistic Regression

5.1 Functions for logistic regression
"""

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_log_loss(y, y_pred):
    # to avoid log(0)
    eps = 1e-15
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))

def compute_log_gradient(X, y, y_pred):
    m = len(y)
    dw = (1/m) * X.T.dot(y_pred - y)
    db = (1/m) * np.sum(y_pred - y)
    return dw, db

"""5.2 Gradient Descent for Logistic Regression"""

def logistic_regression_gd(X, y, lr=0.01, epochs=1000):
    n_features = X.shape[1]
    w = np.zeros(n_features)
    b = 0
    losses = []

    for i in range(epochs):
        z = X.dot(w) + b
        y_pred = sigmoid(z)
        loss = compute_log_loss(y, y_pred)
        losses.append(loss)
        dw, db = compute_log_gradient(X, y, y_pred)
        w -= lr * dw
        b -= lr * db

    return w, b, losses

"""5.3. Model training"""

w_log, b_log, losses_log = logistic_regression_gd(X_train, y_train, lr=0.1, epochs=1000)

"""5.4. Graph of the loss function"""

plt.plot(losses_log)
plt.xlabel("Epochs")
plt.ylabel("Log Loss")
plt.title("Convergence of gradient descent (logistic regression)")
plt.show()

"""5.5. Predictions on the test sample"""

y_test_pred_prob = sigmoid(X_test.dot(w_log) + b_log)
y_test_pred = (y_test_pred_prob >= 0.5).astype(int)

"""5.6. Quality Metrics (sklearn)"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

accuracy = accuracy_score(y_test, y_test_pred)
precision = precision_score(y_test, y_test_pred)
recall = recall_score(y_test, y_test_pred)
f1 = f1_score(y_test, y_test_pred)
conf_matrix = confusion_matrix(y_test, y_test_pred)
roc_auc = roc_auc_score(y_test, y_test_pred_prob)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")
print("Confusion Matrix:\n", conf_matrix)

"""**Chapter 6.** Additional Model - DECISION TREE

6.1. Creating and training a decision tree
"""

from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)  # depth limitation to avoid overfitting
dt_model.fit(X_train, y_train)

"""6.2. Predictions on the test sample"""

y_dt_pred = dt_model.predict(X_test)
y_dt_pred_prob = dt_model.predict_proba(X_test)[:,1]  # for ROC AUC

"""6.3. Quality Metrics"""

accuracy_dt = accuracy_score(y_test, y_dt_pred)
precision_dt = precision_score(y_test, y_dt_pred)
recall_dt = recall_score(y_test, y_dt_pred)
f1_dt = f1_score(y_test, y_dt_pred)
roc_auc_dt = roc_auc_score(y_test, y_dt_pred_prob)
conf_matrix_dt = confusion_matrix(y_test, y_dt_pred)

print("Decision Tree Metrics:")
print(f"Accuracy: {accuracy_dt:.4f}")
print(f"Precision: {precision_dt:.4f}")
print(f"Recall: {recall_dt:.4f}")
print(f"F1-score: {f1_dt:.4f}")
print(f"ROC AUC: {roc_auc_dt:.4f}")
print("Confusion Matrix:\n", conf_matrix_dt)

"""6.4 Visualizing the Decision Tree (optional)"""

from sklearn.tree import plot_tree

plt.figure(figsize=(15,8))
plot_tree(dt_model, feature_names=df.columns[:-1], class_names=["No CHD","CHD"], filled=True)
plt.show()

"""**Chapter 7.** Interactive Interface"""

from ipywidgets import interact, IntSlider, FloatSlider, Dropdown
import matplotlib.pyplot as plt

"""7.1 Create interactive interface"""

feature_names = df.drop('TenYearCHD', axis=1).columns

def run_model(model_type="Logistic Regression", lr=0.1, epochs=500):

    if model_type == "Linear Regression":
        w, b, losses = linear_regression_gd(X_train, y_train, lr=lr, epochs=epochs)
        y_pred = X_test.dot(w) + b

        # MSE
        mse = np.mean((y_test - y_pred) ** 2)

        # Loss graph
        plt.figure(figsize=(6,4))
        plt.plot(losses)
        plt.xlabel("Epochs")
        plt.ylabel("MSE")
        plt.title("Linear Regression — MSE")
        plt.show()

        print(f"MSE: {mse:.4f}")

    elif model_type == "Logistic Regression":
        w, b, losses = logistic_regression_gd(X_train, y_train, lr=lr, epochs=epochs)
        y_prob = sigmoid(X_test.dot(w) + b)
        y_pred = (y_prob >= 0.5).astype(int)

        # Log Loss graph
        plt.figure(figsize=(6,4))
        plt.plot(losses)
        plt.xlabel("Epochs")
        plt.ylabel("Log Loss")
        plt.title("Logistic Regression — Log Loss")
        plt.show()

        print("Accuracy:", accuracy_score(y_test, y_pred))
        print("Precision:", precision_score(y_test, y_pred))
        print("Recall:", recall_score(y_test, y_pred))
        print("F1-score:", f1_score(y_test, y_pred))
        print("ROC AUC:", roc_auc_score(y_test, y_prob))
        print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    else:
        dt = DecisionTreeClassifier(max_depth=5, random_state=42)
        dt.fit(X_train, y_train)

        y_pred = dt.predict(X_test)
        y_prob = dt.predict_proba(X_test)[:, 1]

        # feature importance graph
        importances = dt.feature_importances_
        indices = np.argsort(importances)[::-1]

        plt.figure(figsize=(7,4))
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(
             range(len(importances)),
             feature_names[indices],
             rotation=90
        )
        plt.title("Decision Tree — Feature Importances")
        plt.show()

        print("Accuracy:", accuracy_score(y_test, y_pred))
        print("Precision:", precision_score(y_test, y_pred))
        print("Recall:", recall_score(y_test, y_pred))
        print("F1-score:", f1_score(y_test, y_pred))
        print("ROC AUC:", roc_auc_score(y_test, y_prob))
        print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

interact(
    run_model,
    model_type=Dropdown(
        options=["Linear Regression", "Logistic Regression", "Decision Tree"],
        value="Logistic Regression",
        description="Model"
    ),
    lr=FloatSlider(
        min=0.001, max=1.0, step=0.001, value=0.1,
        description="Learning rate"
    ),
    epochs=IntSlider(
        min=100, max=2000, step=100, value=500,
        description="Epochs"
    )
)

"""**Chapter 8.** Experiment with Hyperparameters

8.1. Experiment with learning rate (logistic regression)
"""

learning_rates = [0.001, 0.01, 0.1, 0.5]
plt.figure(figsize=(8,5))

for lr in learning_rates:
    _, _, losses = logistic_regression_gd(X_train, y_train, lr=lr, epochs=500)
    plt.plot(losses, label=f"lr={lr}")

plt.xlabel("Epochs")
plt.ylabel("Log Loss")
plt.title("The influence of learning rate on convergence")
plt.legend()
plt.show()

"""8.2 Experiment with the number of epochs"""

epochs_list = [100, 500, 1000, 1500]
plt.figure(figsize=(8,5))

for epochs in epochs_list:
    _, _, losses = logistic_regression_gd(X_train, y_train, lr=0.1, epochs=epochs)
    plt.plot(losses, label=f"epochs={epochs}")

plt.xlabel("Epochs")
plt.ylabel("Log Loss")
plt.title("The influence of the number of epochs on convergence")
plt.legend()
plt.show()